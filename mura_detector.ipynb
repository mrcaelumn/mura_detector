{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "mura_detector.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ISLP7e8o6ZDK"
   },
   "source": [
    "import PIL\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "IMG_H = 128\n",
    "IMG_W = 128\n",
    "IMG_C = 3  ## Change this to 1 for grayscale.\n",
    "\n",
    "\n",
    "# Regularization Rate for each loss function\n",
    "\n",
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Loss function for evaluating adversarial loss\n",
    "adv_loss_fn = tf.losses.MeanSquaredError()\n",
    "\n",
    "w_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QjzYjrZEp3k9"
   },
   "source": [
    "def load_image(image_path):\n",
    "  img = tf.io.read_file(image_path)\n",
    "  img = tf.io.decode_bmp(img)\n",
    "  img = tf.image.resize_with_crop_or_pad(img, IMG_H, IMG_W)\n",
    "  img = tf.cast(img, tf.float32)\n",
    "  img = (img - 127.5) / 127.5\n",
    "  return img"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QT7glm2zsyYk"
   },
   "source": [
    "def tf_dataset(images_path, batch_size):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(images_path)\n",
    "  dataset = dataset.shuffle(buffer_size=10240)\n",
    "  dataset = dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "  return dataset"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xFL24bEX65GT"
   },
   "source": [
    "def conv_block(input, num_filters):\n",
    "    x = tf.keras.layers.Conv2D(num_filters, kernel_size=(1,1), padding=\"same\")(input)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(num_filters, kernel_size=(3,3), padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "    return x"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aPVAOjNg69AQ"
   },
   "source": [
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = tf.keras.layers.Conv2DTranspose(num_filters, (1, 1), strides=2, padding=\"same\")(input)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bm_XokrmFnlN"
   },
   "source": [
    "class ResUnetGAN(tf.keras.models.Model):\n",
    "    def __init__(self, input_shape, batch_size):\n",
    "        super(ResUnetGAN, self).__init__()\n",
    "        self.discriminator = self.build_discriminator(input_shape)\n",
    "        self.generator = self.build_generator_resnet50_unet(input_shape)\n",
    "        self.batch_size = batch_size\n",
    "        self.ADV_REG_RATE_LF = 1\n",
    "        self.REC_REG_RATE_LF = 50\n",
    "        self.SSIM_REG_RATE_LF = 50\n",
    "        self.FEAT_REG_RATE_LF = 1\n",
    "\n",
    "        # self.discriminator.summary()\n",
    "        # self.generator.summary()\n",
    "\n",
    "    # create generator model based on resnet50 and unet network\n",
    "    def build_generator_resnet50_unet(self, input_shape):\n",
    "        # print(inputs)\n",
    "        # print(\"pretained start\")\n",
    "        \"\"\" Pre-trained ResNet50 Model \"\"\"\n",
    "        resnet50 = tf.keras.applications.ResNet50(include_top=False, weights=\"imagenet\", input_tensor=input_shape)\n",
    "        # print(\"testing\")\n",
    "        \"\"\" Encoder using resnet50\"\"\"\n",
    "        # for layer in resnet50.layers:\n",
    "        #   print(layer.name)\n",
    "        s1 = resnet50.get_layer(\"input_1\").output           ## (128 x 128)\n",
    "        # print(s1)\n",
    "        s2 = resnet50.get_layer(\"conv1_relu\").output        ## (64 x 64)\n",
    "        s3 = resnet50.get_layer(\"conv2_block3_out\").output  ## (32 x 32)\n",
    "        s4 = resnet50.get_layer(\"conv3_block4_out\").output  ## (16 x 16)\n",
    "\n",
    "        \"\"\" Bridge \"\"\"\n",
    "        b1 = resnet50.get_layer(\"conv4_block6_out\").output  ## (32 x 32)\n",
    "\n",
    "        # print(\"test\")\n",
    "        # print(b1.get_weights())\n",
    "        \"\"\" Decoder unet\"\"\"\n",
    "        d1 = decoder_block(b1, s4, 128)                     ## (16 x 16)\n",
    "        d2 = decoder_block(d1, s3, 64)                     ## (32 x 32)\n",
    "        d3 = decoder_block(d2, s2, 32)                     ## (64 x 64)\n",
    "        d4 = decoder_block(d3, s1, 16)                      ## (128 x 128)\n",
    "\n",
    "        \"\"\" Output \"\"\"\n",
    "        final_model = tf.keras.layers.Conv2D(3, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs, outputs=[final_model, b1])\n",
    "\n",
    "        return model\n",
    "        # return outputs\n",
    "\n",
    "    # create discriminator model\n",
    "\n",
    "    def build_discriminator(self ,input_shape):\n",
    "      # Load the pre-trained model and freeze it.\n",
    "\n",
    "\n",
    "        x = tf.keras.layers.SeparableConvolution2D(32,kernel_size= (1, 1), strides=(2, 2), padding='same')(input_shape)\n",
    "        x = tf.keras.layers.LeakyReLU()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "        x = tf.keras.layers.SeparableConvolution2D(64,kernel_size=(1, 1), strides=(2, 2), padding='same')(x)\n",
    "        x = tf.keras.layers.LeakyReLU()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs, x)\n",
    "        return model\n",
    "        # return x\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer):\n",
    "        super(ResUnetGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "\n",
    "  \n",
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "    @tf.function\n",
    "    def train_step(self, images):\n",
    "\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            if tf.math.is_nan(images) is not None:\n",
    "                tf.print(tf.math.is_nan(images),\" images is NaN\")\n",
    "            else:\n",
    "                tf.print(images)\n",
    "            reconstructed_images, low_feature = self.generator(images, training=True)\n",
    "            real_output = self.discriminator(images, training=True)\n",
    "            # print(generated_images.shape)\n",
    "            fake_output = self.discriminator(reconstructed_images, training=True)\n",
    "\n",
    "            # if tf.math.is_nan(real_output) is not None:\n",
    "            #     tf.print(tf.math.is_nan(real_output),\" real_output is NaN: \", real_output)\n",
    "            # else:\n",
    "            #     tf.print(real_output)\n",
    "            #\n",
    "            # if tf.math.is_nan(fake_output) is not None:\n",
    "            #     tf.print(tf.math.is_nan(fake_output),\" fake_output is NaN: \", fake_output)\n",
    "            # else:\n",
    "            #     tf.print(fake_output)\n",
    "            #\n",
    "            # if tf.math.is_nan(reconstructed_images) is not None:\n",
    "            #     tf.print(tf.math.is_nan(reconstructed_images),\" reconstructed_images is NaN: \",reconstructed_images)\n",
    "            # else:\n",
    "            #     tf.print(reconstructed_images)\n",
    "\n",
    "            # Loss 1: ADVERSARIAL loss\n",
    "            loss_adv = tf.reduce_mean(tf.math.log(real_output) + tf.math.log(1 - fake_output))\n",
    "            # Loss 2: RECONSTRUCTION loss\n",
    "            loss_rec = tf.math.reduce_sum(tf.math.abs(images - reconstructed_images), keepdims=True)\n",
    "            # Loss 3: SSIM loss\n",
    "            loss_ssim = 1 - tf.image.ssim(images,reconstructed_images, max_val=1.0)[0]\n",
    "            # Loss 4: FEATURE loss\n",
    "            loss_feat = tf.math.reduce_sum(tf.math.square(real_output - fake_output), keepdims=True)\n",
    "\n",
    "            gen_loss = tf.math.reduce_mean((loss_adv * self.ADV_REG_RATE_LF) + (loss_rec * self.REC_REG_RATE_LF) + (loss_ssim * self.SSIM_REG_RATE_LF) + (loss_feat * self.FEAT_REG_RATE_LF))\n",
    "            disc_loss = tf.math.reduce_mean((loss_adv * self.ADV_REG_RATE_LF) + (loss_feat * self.FEAT_REG_RATE_LF))\n",
    "\n",
    "        \n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "        \n",
    "        self.g_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "        self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "\n",
    "        self.mean_loss_adv = tf.math.reduce_mean(loss_adv)\n",
    "        self.mean_loss_rec = tf.math.reduce_mean(loss_rec)\n",
    "        self.mean_loss_ssim = tf.math.reduce_mean(loss_ssim)\n",
    "        self.mean_loss_feat = tf.math.reduce_mean(loss_feat)\n",
    "\n",
    "        # tf.summary.scalar('loss_adv', self.mean_loss_adv)\n",
    "        # tf.summary.scalar('loss_rec', self.mean_loss_rec)\n",
    "        # tf.summary.scalar('loss_ssim', self.mean_loss_ssim)\n",
    "        # tf.summary.scalar('loss_feat', self.mean_loss_feat)\n",
    "        # tf.summary.scalar('gen_loss', gen_loss)\n",
    "        # tf.summary.scalar('disc_loss', disc_loss)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"gen_loss\": gen_loss,\n",
    "            \"disc_loss\": disc_loss,\n",
    "            \"loss_adv\": self.mean_loss_adv,\n",
    "            \"loss_rec\": self.mean_loss_rec,\n",
    "            \"loss_ssim\": self.mean_loss_ssim,\n",
    "            \"loss_feat\": self.mean_loss_feat\n",
    "        }\n",
    "\n",
    "    def saved_model(self, filepath, num_of_epoch):\n",
    "        self.generator.save(filepath + \"g_model\" + str(num_of_epoch) + \".h5\")\n",
    "        self.discriminator.save(filepath + \"d_model\" + str(num_of_epoch) + \".h5\")\n",
    "\n",
    "    def loaded_model(self, filepath):\n",
    "        self.generator.load_weights(filepath)\n",
    "        self.discriminator.load_weights(filepath)"
   ],
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KoSI9-4-tVt",
    "outputId": "bc609879-a7d8-427b-8390-9df4c65d51eb"
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # run the function here\n",
    "    print(\"start\")\n",
    "    ## Hyperparameters\n",
    "    batch_size = 24\n",
    "    input_shape = (IMG_W, IMG_H, IMG_C)\n",
    "    # print(input_shape)\n",
    "\n",
    "    \"\"\" Input \"\"\"\n",
    "    inputs = tf.keras.layers.Input(input_shape, name=\"input_1\")\n",
    "\n",
    "    num_epochs = 600\n",
    "    train_images_path = glob(\"/Users/mrcaelumn/YZU/projects/mura_detector/mura_data/mura_data/train_data/*.bmp\")\n",
    "\n",
    "\n",
    "    # d_model = build_discriminator(inputs)\n",
    "    # g_model = build_generator_resnet50_unet(inputs)\n",
    "\n",
    "    resunetgan = ResUnetGAN(inputs, batch_size)\n",
    "\n",
    "\n",
    "    g_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5, beta_2=0.999)\n",
    "    d_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5, beta_2=0.999)\n",
    "    resunetgan.compile(d_optimizer, g_optimizer)\n",
    "\n",
    "    # print(train_images_path)\n",
    "    train_images_dataset = tf_dataset(train_images_path, batch_size)\n",
    "\n",
    "    # resunetgan.fit(train_images_dataset)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: \", epoch)\n",
    "        start = time.time()\n",
    "        for image_batch in train_images_dataset:\n",
    "        # print(image_batch.shape)\n",
    "\n",
    "            print(\"Images_batch: \", image_batch)\n",
    "            resunetgan.fit(image_batch)\n",
    "            resunetgan.saved_model(\"mura_data/saved_model/\", num_epochs)\n",
    "\n",
    "    # resunetgan.summary()\n",
    "    # resunetgan.save_weights(\"saved_model/resunet_model\")\n"
   ],
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Epoch:  0\n",
      "Images_batch:  tf.Tensor(\n",
      "[[[[-0.5921569  -0.05882353 -0.08235294]\n",
      "   [-0.5921569  -0.06666667 -0.09019608]\n",
      "   [-0.5921569  -0.06666667 -0.06666667]\n",
      "   ...\n",
      "   [-0.60784316 -0.09019608 -0.11372549]\n",
      "   [-0.6        -0.09803922 -0.12156863]\n",
      "   [-0.62352943 -0.09803922 -0.10588235]]\n",
      "\n",
      "  [[-0.5921569  -0.05882353 -0.05882353]\n",
      "   [-0.5921569  -0.05882353 -0.05098039]\n",
      "   [-0.58431375 -0.03529412 -0.06666667]\n",
      "   ...\n",
      "   [-0.6        -0.08235294 -0.12156863]\n",
      "   [-0.60784316 -0.09803922 -0.12156863]\n",
      "   [-0.60784316 -0.09803922 -0.11372549]]\n",
      "\n",
      "  [[-0.6        -0.05882353 -0.07450981]\n",
      "   [-0.5921569  -0.05882353 -0.06666667]\n",
      "   [-0.58431375 -0.06666667 -0.05882353]\n",
      "   ...\n",
      "   [-0.6156863  -0.09803922 -0.12941177]\n",
      "   [-0.6        -0.09019608 -0.12941177]\n",
      "   [-0.62352943 -0.09019608 -0.12156863]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.5921569  -0.06666667 -0.06666667]\n",
      "   [-0.58431375 -0.08235294 -0.05882353]\n",
      "   [-0.5921569  -0.07450981 -0.06666667]\n",
      "   ...\n",
      "   [-0.6156863  -0.11372549 -0.10588235]\n",
      "   [-0.6        -0.11372549 -0.11372549]\n",
      "   [-0.60784316 -0.12156863 -0.10588235]]\n",
      "\n",
      "  [[-0.5921569  -0.08235294 -0.07450981]\n",
      "   [-0.5921569  -0.07450981 -0.05098039]\n",
      "   [-0.5921569  -0.09019608 -0.05882353]\n",
      "   ...\n",
      "   [-0.60784316 -0.09019608 -0.11372549]\n",
      "   [-0.60784316 -0.11372549 -0.10588235]\n",
      "   [-0.60784316 -0.10588235 -0.10588235]]\n",
      "\n",
      "  [[-0.6        -0.07450981 -0.08235294]\n",
      "   [-0.5921569  -0.07450981 -0.07450981]\n",
      "   [-0.58431375 -0.08235294 -0.06666667]\n",
      "   ...\n",
      "   [-0.60784316 -0.10588235 -0.10588235]\n",
      "   [-0.60784316 -0.10588235 -0.12156863]\n",
      "   [-0.60784316 -0.10588235 -0.11372549]]]\n",
      "\n",
      "\n",
      " [[[-0.5921569  -0.05098039 -0.07450981]\n",
      "   [-0.5921569  -0.04313726 -0.05882353]\n",
      "   [-0.58431375 -0.05882353 -0.05882353]\n",
      "   ...\n",
      "   [-0.60784316 -0.09803922 -0.10588235]\n",
      "   [-0.60784316 -0.08235294 -0.12941177]\n",
      "   [-0.60784316 -0.09019608 -0.11372549]]\n",
      "\n",
      "  [[-0.6        -0.04313726 -0.07450981]\n",
      "   [-0.5921569  -0.05098039 -0.05098039]\n",
      "   [-0.5921569  -0.05098039 -0.08235294]\n",
      "   ...\n",
      "   [-0.6156863  -0.08235294 -0.10588235]\n",
      "   [-0.60784316 -0.07450981 -0.10588235]\n",
      "   [-0.60784316 -0.09803922 -0.11372549]]\n",
      "\n",
      "  [[-0.58431375 -0.05882353 -0.07450981]\n",
      "   [-0.6        -0.06666667 -0.05098039]\n",
      "   [-0.6        -0.05098039 -0.06666667]\n",
      "   ...\n",
      "   [-0.60784316 -0.09019608 -0.11372549]\n",
      "   [-0.6        -0.09019608 -0.10588235]\n",
      "   [-0.6156863  -0.07450981 -0.10588235]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.6        -0.07450981 -0.05882353]\n",
      "   [-0.6        -0.05882353 -0.06666667]\n",
      "   [-0.5921569  -0.05098039 -0.04313726]\n",
      "   ...\n",
      "   [-0.6        -0.09019608 -0.09803922]\n",
      "   [-0.6        -0.09019608 -0.09019608]\n",
      "   [-0.6        -0.09803922 -0.09803922]]\n",
      "\n",
      "  [[-0.60784316 -0.08235294 -0.07450981]\n",
      "   [-0.5921569  -0.05882353 -0.05882353]\n",
      "   [-0.5764706  -0.05882353 -0.05098039]\n",
      "   ...\n",
      "   [-0.6        -0.07450981 -0.09019608]\n",
      "   [-0.6        -0.09019608 -0.09803922]\n",
      "   [-0.6        -0.08235294 -0.08235294]]\n",
      "\n",
      "  [[-0.6        -0.05882353 -0.06666667]\n",
      "   [-0.58431375 -0.04313726 -0.05098039]\n",
      "   [-0.5921569  -0.03529412 -0.05882353]\n",
      "   ...\n",
      "   [-0.6        -0.08235294 -0.09019608]\n",
      "   [-0.6        -0.06666667 -0.09019608]\n",
      "   [-0.6        -0.07450981 -0.09803922]]]\n",
      "\n",
      "\n",
      " [[[-0.5921569  -0.05098039 -0.10588235]\n",
      "   [-0.58431375 -0.05098039 -0.11372549]\n",
      "   [-0.5921569  -0.04313726 -0.12941177]\n",
      "   ...\n",
      "   [-0.58431375 -0.05882353 -0.14509805]\n",
      "   [-0.58431375 -0.06666667 -0.12941177]\n",
      "   [-0.5921569  -0.05098039 -0.12941177]]\n",
      "\n",
      "  [[-0.5921569  -0.05882353 -0.13725491]\n",
      "   [-0.5921569  -0.06666667 -0.12941177]\n",
      "   [-0.5921569  -0.04313726 -0.11372549]\n",
      "   ...\n",
      "   [-0.6        -0.04313726 -0.12941177]\n",
      "   [-0.6        -0.07450981 -0.12156863]\n",
      "   [-0.58431375 -0.05882353 -0.13725491]]\n",
      "\n",
      "  [[-0.6        -0.05882353 -0.11372549]\n",
      "   [-0.5921569  -0.05098039 -0.12156863]\n",
      "   [-0.5921569  -0.05098039 -0.12156863]\n",
      "   ...\n",
      "   [-0.5921569  -0.07450981 -0.13725491]\n",
      "   [-0.5921569  -0.06666667 -0.14509805]\n",
      "   [-0.58431375 -0.06666667 -0.12941177]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.5921569  -0.05098039 -0.09019608]\n",
      "   [-0.58431375 -0.05098039 -0.09803922]\n",
      "   [-0.58431375 -0.04313726 -0.09803922]\n",
      "   ...\n",
      "   [-0.58431375 -0.04313726 -0.09803922]\n",
      "   [-0.58431375 -0.05098039 -0.12156863]\n",
      "   [-0.58431375 -0.06666667 -0.11372549]]\n",
      "\n",
      "  [[-0.5764706  -0.05098039 -0.12156863]\n",
      "   [-0.5921569  -0.05882353 -0.09803922]\n",
      "   [-0.58431375 -0.05882353 -0.09019608]\n",
      "   ...\n",
      "   [-0.5764706  -0.03529412 -0.10588235]\n",
      "   [-0.58431375 -0.03529412 -0.09803922]\n",
      "   [-0.5921569  -0.05098039 -0.11372549]]\n",
      "\n",
      "  [[-0.58431375 -0.05882353 -0.09803922]\n",
      "   [-0.58431375 -0.05882353 -0.09019608]\n",
      "   [-0.5921569  -0.05882353 -0.09803922]\n",
      "   ...\n",
      "   [-0.58431375 -0.03529412 -0.09803922]\n",
      "   [-0.58431375 -0.04313726 -0.09803922]\n",
      "   [-0.58431375 -0.03529412 -0.10588235]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-0.5921569  -0.09019608 -0.10588235]\n",
      "   [-0.5921569  -0.09019608 -0.10588235]\n",
      "   [-0.5921569  -0.09019608 -0.09019608]\n",
      "   ...\n",
      "   [-0.6        -0.13725491 -0.12156863]\n",
      "   [-0.6        -0.12156863 -0.12156863]\n",
      "   [-0.60784316 -0.12156863 -0.12156863]]\n",
      "\n",
      "  [[-0.5921569  -0.10588235 -0.10588235]\n",
      "   [-0.5921569  -0.11372549 -0.11372549]\n",
      "   [-0.5921569  -0.09803922 -0.09803922]\n",
      "   ...\n",
      "   [-0.6        -0.10588235 -0.12941177]\n",
      "   [-0.60784316 -0.11372549 -0.12941177]\n",
      "   [-0.6        -0.12156863 -0.12941177]]\n",
      "\n",
      "  [[-0.5921569  -0.10588235 -0.11372549]\n",
      "   [-0.6        -0.09019608 -0.10588235]\n",
      "   [-0.6        -0.09803922 -0.12156863]\n",
      "   ...\n",
      "   [-0.60784316 -0.12156863 -0.14509805]\n",
      "   [-0.6        -0.12156863 -0.13725491]\n",
      "   [-0.6        -0.12156863 -0.12941177]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.5686275  -0.01960784 -0.01960784]\n",
      "   [-0.56078434 -0.02745098 -0.03529412]\n",
      "   [-0.56078434 -0.00392157 -0.03529412]\n",
      "   ...\n",
      "   [-0.5686275  -0.02745098 -0.02745098]\n",
      "   [-0.5764706  -0.01960784 -0.01176471]\n",
      "   [-0.58431375 -0.02745098 -0.01176471]]\n",
      "\n",
      "  [[-0.56078434 -0.00392157 -0.03529412]\n",
      "   [-0.56078434 -0.01960784 -0.01960784]\n",
      "   [-0.5529412   0.00392157 -0.01176471]\n",
      "   ...\n",
      "   [-0.5686275  -0.01960784 -0.01960784]\n",
      "   [-0.58431375 -0.01176471 -0.02745098]\n",
      "   [-0.5686275   0.00392157 -0.01176471]]\n",
      "\n",
      "  [[-0.56078434 -0.00392157 -0.02745098]\n",
      "   [-0.56078434 -0.01176471 -0.04313726]\n",
      "   [-0.56078434 -0.01176471 -0.01960784]\n",
      "   ...\n",
      "   [-0.5686275  -0.01176471 -0.01176471]\n",
      "   [-0.5764706  -0.01960784 -0.02745098]\n",
      "   [-0.5686275   0.00392157 -0.01960784]]]\n",
      "\n",
      "\n",
      " [[[-0.58431375 -0.10588235 -0.13725491]\n",
      "   [-0.5921569  -0.11372549 -0.13725491]\n",
      "   [-0.6        -0.11372549 -0.12941177]\n",
      "   ...\n",
      "   [-0.5764706  -0.10588235 -0.10588235]\n",
      "   [-0.58431375 -0.10588235 -0.11372549]\n",
      "   [-0.5764706  -0.09019608 -0.12156863]]\n",
      "\n",
      "  [[-0.5921569  -0.12156863 -0.12941177]\n",
      "   [-0.5921569  -0.12156863 -0.12156863]\n",
      "   [-0.58431375 -0.09019608 -0.12941177]\n",
      "   ...\n",
      "   [-0.5764706  -0.11372549 -0.13725491]\n",
      "   [-0.5764706  -0.09803922 -0.10588235]\n",
      "   [-0.58431375 -0.11372549 -0.11372549]]\n",
      "\n",
      "  [[-0.5764706  -0.10588235 -0.12156863]\n",
      "   [-0.5921569  -0.09019608 -0.12941177]\n",
      "   [-0.5921569  -0.11372549 -0.12156863]\n",
      "   ...\n",
      "   [-0.5764706  -0.10588235 -0.12156863]\n",
      "   [-0.5764706  -0.09803922 -0.11372549]\n",
      "   [-0.5764706  -0.09803922 -0.10588235]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.58431375 -0.09803922 -0.12156863]\n",
      "   [-0.5764706  -0.09803922 -0.09803922]\n",
      "   [-0.5686275  -0.09803922 -0.09803922]\n",
      "   ...\n",
      "   [-0.5764706  -0.10588235 -0.07450981]\n",
      "   [-0.5686275  -0.07450981 -0.08235294]\n",
      "   [-0.5686275  -0.10588235 -0.08235294]]\n",
      "\n",
      "  [[-0.5764706  -0.09803922 -0.09803922]\n",
      "   [-0.58431375 -0.09803922 -0.10588235]\n",
      "   [-0.5686275  -0.10588235 -0.09803922]\n",
      "   ...\n",
      "   [-0.5764706  -0.09803922 -0.09019608]\n",
      "   [-0.5686275  -0.08235294 -0.08235294]\n",
      "   [-0.5764706  -0.09019608 -0.09019608]]\n",
      "\n",
      "  [[-0.5764706  -0.10588235 -0.09019608]\n",
      "   [-0.5686275  -0.09803922 -0.09803922]\n",
      "   [-0.5764706  -0.07450981 -0.10588235]\n",
      "   ...\n",
      "   [-0.5764706  -0.09019608 -0.09019608]\n",
      "   [-0.5764706  -0.09803922 -0.09803922]\n",
      "   [-0.5686275  -0.08235294 -0.08235294]]]\n",
      "\n",
      "\n",
      " [[[-0.5921569  -0.12941177 -0.13725491]\n",
      "   [-0.5921569  -0.13725491 -0.12156863]\n",
      "   [-0.58431375 -0.12156863 -0.11372549]\n",
      "   ...\n",
      "   [-0.6        -0.16078432 -0.16862746]\n",
      "   [-0.60784316 -0.15294118 -0.15294118]\n",
      "   [-0.6        -0.14509805 -0.1764706 ]]\n",
      "\n",
      "  [[-0.5921569  -0.12941177 -0.12156863]\n",
      "   [-0.5921569  -0.11372549 -0.10588235]\n",
      "   [-0.5921569  -0.11372549 -0.11372549]\n",
      "   ...\n",
      "   [-0.6        -0.15294118 -0.16862746]\n",
      "   [-0.6        -0.16078432 -0.16862746]\n",
      "   [-0.6        -0.15294118 -0.18431373]]\n",
      "\n",
      "  [[-0.5921569  -0.11372549 -0.11372549]\n",
      "   [-0.5921569  -0.10588235 -0.12156863]\n",
      "   [-0.58431375 -0.12156863 -0.12156863]\n",
      "   ...\n",
      "   [-0.5921569  -0.16078432 -0.18431373]\n",
      "   [-0.6        -0.15294118 -0.18431373]\n",
      "   [-0.60784316 -0.16862746 -0.16078432]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.5764706  -0.09019608 -0.09019608]\n",
      "   [-0.5686275  -0.09803922 -0.09803922]\n",
      "   [-0.5764706  -0.09803922 -0.09803922]\n",
      "   ...\n",
      "   [-0.58431375 -0.11372549 -0.12941177]\n",
      "   [-0.58431375 -0.11372549 -0.11372549]\n",
      "   [-0.58431375 -0.10588235 -0.11372549]]\n",
      "\n",
      "  [[-0.5764706  -0.10588235 -0.10588235]\n",
      "   [-0.5764706  -0.09019608 -0.09803922]\n",
      "   [-0.5686275  -0.09803922 -0.09019608]\n",
      "   ...\n",
      "   [-0.5921569  -0.12941177 -0.13725491]\n",
      "   [-0.58431375 -0.12941177 -0.12156863]\n",
      "   [-0.58431375 -0.11372549 -0.12941177]]\n",
      "\n",
      "  [[-0.5764706  -0.10588235 -0.09803922]\n",
      "   [-0.58431375 -0.09019608 -0.09803922]\n",
      "   [-0.5764706  -0.09019608 -0.10588235]\n",
      "   ...\n",
      "   [-0.58431375 -0.12941177 -0.12156863]\n",
      "   [-0.5764706  -0.12941177 -0.13725491]\n",
      "   [-0.58431375 -0.13725491 -0.12156863]]]], shape=(24, 128, 128, 3), dtype=float32)\n",
      "[[[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]]]  images is NaN\n",
      "1/1 [==============================] - 13s 13s/step - gen_loss: nan - disc_loss: nan - loss_adv: nan - loss_rec: 961180.7500 - loss_ssim: 1.0368 - loss_feat: 0.0103\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mrcaelumn/YZU/.jupyter_env/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Images_batch:  tf.Tensor(\n",
      "[[[[-0.60784316 -0.09803922 -0.10588235]\n",
      "   [-0.60784316 -0.11372549 -0.10588235]\n",
      "   [-0.5921569  -0.10588235 -0.10588235]\n",
      "   ...\n",
      "   [-0.5921569  -0.10588235 -0.12941177]\n",
      "   [-0.5921569  -0.09803922 -0.11372549]\n",
      "   [-0.5921569  -0.09019608 -0.12156863]]\n",
      "\n",
      "  [[-0.60784316 -0.09803922 -0.11372549]\n",
      "   [-0.60784316 -0.10588235 -0.11372549]\n",
      "   [-0.6156863  -0.10588235 -0.10588235]\n",
      "   ...\n",
      "   [-0.5921569  -0.07450981 -0.11372549]\n",
      "   [-0.5921569  -0.10588235 -0.12156863]\n",
      "   [-0.5921569  -0.10588235 -0.12156863]]\n",
      "\n",
      "  [[-0.60784316 -0.09803922 -0.12156863]\n",
      "   [-0.6        -0.09803922 -0.10588235]\n",
      "   [-0.6        -0.09803922 -0.11372549]\n",
      "   ...\n",
      "   [-0.5921569  -0.10588235 -0.12941177]\n",
      "   [-0.5921569  -0.08235294 -0.13725491]\n",
      "   [-0.5921569  -0.10588235 -0.12156863]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.5921569  -0.07450981 -0.05882353]\n",
      "   [-0.5764706  -0.06666667 -0.05882353]\n",
      "   [-0.5764706  -0.07450981 -0.05882353]\n",
      "   ...\n",
      "   [-0.5764706  -0.06666667 -0.05882353]\n",
      "   [-0.58431375 -0.07450981 -0.08235294]\n",
      "   [-0.58431375 -0.08235294 -0.07450981]]\n",
      "\n",
      "  [[-0.58431375 -0.08235294 -0.06666667]\n",
      "   [-0.5764706  -0.05098039 -0.06666667]\n",
      "   [-0.58431375 -0.05098039 -0.04313726]\n",
      "   ...\n",
      "   [-0.5764706  -0.03529412 -0.07450981]\n",
      "   [-0.58431375 -0.05098039 -0.07450981]\n",
      "   [-0.58431375 -0.07450981 -0.07450981]]\n",
      "\n",
      "  [[-0.58431375 -0.06666667 -0.06666667]\n",
      "   [-0.58431375 -0.07450981 -0.06666667]\n",
      "   [-0.58431375 -0.06666667 -0.05098039]\n",
      "   ...\n",
      "   [-0.5764706  -0.05882353 -0.08235294]\n",
      "   [-0.58431375 -0.06666667 -0.08235294]\n",
      "   [-0.5764706  -0.07450981 -0.07450981]]]\n",
      "\n",
      "\n",
      " [[[-0.5764706  -0.12156863 -0.14509805]\n",
      "   [-0.58431375 -0.12941177 -0.13725491]\n",
      "   [-0.5921569  -0.10588235 -0.14509805]\n",
      "   ...\n",
      "   [-0.60784316 -0.16078432 -0.1764706 ]\n",
      "   [-0.6        -0.14509805 -0.1764706 ]\n",
      "   [-0.60784316 -0.15294118 -0.18431373]]\n",
      "\n",
      "  [[-0.5764706  -0.11372549 -0.14509805]\n",
      "   [-0.58431375 -0.09803922 -0.13725491]\n",
      "   [-0.5764706  -0.11372549 -0.14509805]\n",
      "   ...\n",
      "   [-0.6        -0.15294118 -0.16078432]\n",
      "   [-0.6        -0.15294118 -0.1764706 ]\n",
      "   [-0.5921569  -0.16078432 -0.16078432]]\n",
      "\n",
      "  [[-0.58431375 -0.12156863 -0.11372549]\n",
      "   [-0.58431375 -0.11372549 -0.13725491]\n",
      "   [-0.6        -0.12941177 -0.14509805]\n",
      "   ...\n",
      "   [-0.60784316 -0.15294118 -0.18431373]\n",
      "   [-0.6        -0.16078432 -0.1764706 ]\n",
      "   [-0.6        -0.16078432 -0.16862746]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.5764706  -0.08235294 -0.11372549]\n",
      "   [-0.58431375 -0.09803922 -0.09803922]\n",
      "   [-0.58431375 -0.10588235 -0.11372549]\n",
      "   ...\n",
      "   [-0.6        -0.11372549 -0.15294118]\n",
      "   [-0.5921569  -0.11372549 -0.14509805]\n",
      "   [-0.5921569  -0.10588235 -0.11372549]]\n",
      "\n",
      "  [[-0.5921569  -0.09803922 -0.13725491]\n",
      "   [-0.5764706  -0.09803922 -0.09803922]\n",
      "   [-0.5764706  -0.08235294 -0.09803922]\n",
      "   ...\n",
      "   [-0.5921569  -0.11372549 -0.13725491]\n",
      "   [-0.60784316 -0.12156863 -0.12941177]\n",
      "   [-0.5921569  -0.11372549 -0.12941177]]\n",
      "\n",
      "  [[-0.58431375 -0.09019608 -0.11372549]\n",
      "   [-0.58431375 -0.10588235 -0.11372549]\n",
      "   [-0.5764706  -0.09803922 -0.11372549]\n",
      "   ...\n",
      "   [-0.6        -0.12156863 -0.12941177]\n",
      "   [-0.6        -0.09019608 -0.13725491]\n",
      "   [-0.6        -0.09803922 -0.12941177]]]\n",
      "\n",
      "\n",
      " [[[-0.6156863  -0.11372549 -0.15294118]\n",
      "   [-0.62352943 -0.10588235 -0.15294118]\n",
      "   [-0.62352943 -0.12156863 -0.14509805]\n",
      "   ...\n",
      "   [-0.62352943 -0.12156863 -0.16078432]\n",
      "   [-0.62352943 -0.12941177 -0.16862746]\n",
      "   [-0.62352943 -0.12941177 -0.16862746]]\n",
      "\n",
      "  [[-0.6156863  -0.12941177 -0.16862746]\n",
      "   [-0.6156863  -0.11372549 -0.16078432]\n",
      "   [-0.6156863  -0.10588235 -0.16078432]\n",
      "   ...\n",
      "   [-0.62352943 -0.10588235 -0.16078432]\n",
      "   [-0.62352943 -0.12156863 -0.1764706 ]\n",
      "   [-0.6156863  -0.12941177 -0.16078432]]\n",
      "\n",
      "  [[-0.60784316 -0.12156863 -0.14509805]\n",
      "   [-0.6156863  -0.11372549 -0.16078432]\n",
      "   [-0.6156863  -0.10588235 -0.16862746]\n",
      "   ...\n",
      "   [-0.6156863  -0.12156863 -0.16862746]\n",
      "   [-0.62352943 -0.12941177 -0.16078432]\n",
      "   [-0.62352943 -0.12941177 -0.1764706 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.60784316 -0.10588235 -0.13725491]\n",
      "   [-0.60784316 -0.10588235 -0.12156863]\n",
      "   [-0.60784316 -0.10588235 -0.12156863]\n",
      "   ...\n",
      "   [-0.6156863  -0.12941177 -0.15294118]\n",
      "   [-0.6156863  -0.10588235 -0.15294118]\n",
      "   [-0.62352943 -0.12156863 -0.15294118]]\n",
      "\n",
      "  [[-0.60784316 -0.10588235 -0.12156863]\n",
      "   [-0.60784316 -0.11372549 -0.12941177]\n",
      "   [-0.60784316 -0.09803922 -0.14509805]\n",
      "   ...\n",
      "   [-0.6156863  -0.12941177 -0.14509805]\n",
      "   [-0.62352943 -0.11372549 -0.14509805]\n",
      "   [-0.6156863  -0.14509805 -0.15294118]]\n",
      "\n",
      "  [[-0.60784316 -0.11372549 -0.13725491]\n",
      "   [-0.6        -0.10588235 -0.12156863]\n",
      "   [-0.60784316 -0.10588235 -0.13725491]\n",
      "   ...\n",
      "   [-0.6156863  -0.12941177 -0.14509805]\n",
      "   [-0.62352943 -0.12941177 -0.14509805]\n",
      "   [-0.62352943 -0.12941177 -0.15294118]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-0.5686275  -0.08235294 -0.09019608]\n",
      "   [-0.5686275  -0.07450981 -0.07450981]\n",
      "   [-0.56078434 -0.05882353 -0.06666667]\n",
      "   ...\n",
      "   [-0.5686275  -0.09019608 -0.12156863]\n",
      "   [-0.5686275  -0.06666667 -0.08235294]\n",
      "   [-0.5686275  -0.07450981 -0.09803922]]\n",
      "\n",
      "  [[-0.5686275  -0.07450981 -0.07450981]\n",
      "   [-0.5764706  -0.05882353 -0.06666667]\n",
      "   [-0.5686275  -0.04313726 -0.05882353]\n",
      "   ...\n",
      "   [-0.5764706  -0.09803922 -0.09803922]\n",
      "   [-0.5686275  -0.09019608 -0.09019608]\n",
      "   [-0.5686275  -0.08235294 -0.10588235]]\n",
      "\n",
      "  [[-0.5686275  -0.08235294 -0.08235294]\n",
      "   [-0.5686275  -0.03529412 -0.06666667]\n",
      "   [-0.56078434 -0.06666667 -0.06666667]\n",
      "   ...\n",
      "   [-0.56078434 -0.09019608 -0.11372549]\n",
      "   [-0.5686275  -0.07450981 -0.10588235]\n",
      "   [-0.5686275  -0.09019608 -0.10588235]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.56078434 -0.05882353 -0.06666667]\n",
      "   [-0.54509807 -0.07450981 -0.07450981]\n",
      "   [-0.56078434 -0.06666667 -0.05882353]\n",
      "   ...\n",
      "   [-0.56078434 -0.05882353 -0.08235294]\n",
      "   [-0.56078434 -0.08235294 -0.07450981]\n",
      "   [-0.56078434 -0.06666667 -0.09019608]]\n",
      "\n",
      "  [[-0.54509807 -0.04313726 -0.06666667]\n",
      "   [-0.5529412  -0.05098039 -0.05882353]\n",
      "   [-0.5529412  -0.06666667 -0.05882353]\n",
      "   ...\n",
      "   [-0.56078434 -0.07450981 -0.08235294]\n",
      "   [-0.56078434 -0.06666667 -0.08235294]\n",
      "   [-0.56078434 -0.06666667 -0.08235294]]\n",
      "\n",
      "  [[-0.5529412  -0.07450981 -0.06666667]\n",
      "   [-0.5529412  -0.04313726 -0.05098039]\n",
      "   [-0.56078434 -0.04313726 -0.06666667]\n",
      "   ...\n",
      "   [-0.56078434 -0.06666667 -0.07450981]\n",
      "   [-0.5529412  -0.05882353 -0.08235294]\n",
      "   [-0.56078434 -0.08235294 -0.06666667]]]\n",
      "\n",
      "\n",
      " [[[-0.58431375 -0.09803922 -0.08235294]\n",
      "   [-0.58431375 -0.09803922 -0.09019608]\n",
      "   [-0.58431375 -0.08235294 -0.09019608]\n",
      "   ...\n",
      "   [-0.5921569  -0.08235294 -0.08235294]\n",
      "   [-0.5921569  -0.09803922 -0.09803922]\n",
      "   [-0.5921569  -0.11372549 -0.09803922]]\n",
      "\n",
      "  [[-0.6        -0.11372549 -0.10588235]\n",
      "   [-0.5921569  -0.09803922 -0.09019608]\n",
      "   [-0.6        -0.09803922 -0.09019608]\n",
      "   ...\n",
      "   [-0.5921569  -0.10588235 -0.07450981]\n",
      "   [-0.5921569  -0.12156863 -0.10588235]\n",
      "   [-0.5921569  -0.07450981 -0.08235294]]\n",
      "\n",
      "  [[-0.6        -0.09803922 -0.09019608]\n",
      "   [-0.5921569  -0.11372549 -0.08235294]\n",
      "   [-0.6        -0.09803922 -0.09019608]\n",
      "   ...\n",
      "   [-0.5764706  -0.07450981 -0.09803922]\n",
      "   [-0.5921569  -0.09019608 -0.10588235]\n",
      "   [-0.5921569  -0.09019608 -0.08235294]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.5686275  -0.02745098 -0.02745098]\n",
      "   [-0.5686275  -0.03529412 -0.01176471]\n",
      "   [-0.5686275  -0.05098039 -0.02745098]\n",
      "   ...\n",
      "   [-0.5686275  -0.01960784 -0.01176471]\n",
      "   [-0.56078434 -0.02745098 -0.01960784]\n",
      "   [-0.56078434 -0.03529412 -0.01176471]]\n",
      "\n",
      "  [[-0.5686275  -0.04313726 -0.01960784]\n",
      "   [-0.5686275  -0.03529412 -0.03529412]\n",
      "   [-0.5764706  -0.04313726 -0.01960784]\n",
      "   ...\n",
      "   [-0.5529412  -0.03529412 -0.01176471]\n",
      "   [-0.56078434 -0.03529412 -0.00392157]\n",
      "   [-0.5529412  -0.04313726 -0.00392157]]\n",
      "\n",
      "  [[-0.5686275  -0.04313726 -0.04313726]\n",
      "   [-0.5686275  -0.03529412 -0.02745098]\n",
      "   [-0.5686275  -0.04313726 -0.03529412]\n",
      "   ...\n",
      "   [-0.5686275  -0.04313726 -0.01176471]\n",
      "   [-0.56078434 -0.03529412 -0.00392157]\n",
      "   [-0.5529412  -0.01960784 -0.00392157]]]\n",
      "\n",
      "\n",
      " [[[-0.58431375 -0.04313726 -0.08235294]\n",
      "   [-0.58431375 -0.05882353 -0.05882353]\n",
      "   [-0.5764706  -0.05098039 -0.05882353]\n",
      "   ...\n",
      "   [-0.5764706  -0.05882353 -0.07450981]\n",
      "   [-0.58431375 -0.04313726 -0.06666667]\n",
      "   [-0.5764706  -0.04313726 -0.08235294]]\n",
      "\n",
      "  [[-0.58431375 -0.04313726 -0.07450981]\n",
      "   [-0.58431375 -0.05098039 -0.06666667]\n",
      "   [-0.5764706  -0.05098039 -0.08235294]\n",
      "   ...\n",
      "   [-0.5764706  -0.04313726 -0.07450981]\n",
      "   [-0.5764706  -0.03529412 -0.08235294]\n",
      "   [-0.5686275  -0.06666667 -0.07450981]]\n",
      "\n",
      "  [[-0.5921569  -0.05098039 -0.09019608]\n",
      "   [-0.5764706  -0.05882353 -0.08235294]\n",
      "   [-0.58431375 -0.05882353 -0.08235294]\n",
      "   ...\n",
      "   [-0.5764706  -0.04313726 -0.06666667]\n",
      "   [-0.5764706  -0.00392157 -0.08235294]\n",
      "   [-0.5764706  -0.03529412 -0.07450981]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.6        -0.09019608 -0.09019608]\n",
      "   [-0.5921569  -0.09803922 -0.11372549]\n",
      "   [-0.60784316 -0.09019608 -0.10588235]\n",
      "   ...\n",
      "   [-0.5921569  -0.06666667 -0.09019608]\n",
      "   [-0.5764706  -0.06666667 -0.09803922]\n",
      "   [-0.58431375 -0.07450981 -0.09019608]]\n",
      "\n",
      "  [[-0.6        -0.11372549 -0.10588235]\n",
      "   [-0.6156863  -0.09019608 -0.09019608]\n",
      "   [-0.5921569  -0.09019608 -0.09803922]\n",
      "   ...\n",
      "   [-0.58431375 -0.05098039 -0.09803922]\n",
      "   [-0.58431375 -0.05882353 -0.07450981]\n",
      "   [-0.58431375 -0.07450981 -0.09803922]]\n",
      "\n",
      "  [[-0.6        -0.09019608 -0.09803922]\n",
      "   [-0.6        -0.09019608 -0.09803922]\n",
      "   [-0.6        -0.08235294 -0.10588235]\n",
      "   ...\n",
      "   [-0.58431375 -0.05882353 -0.06666667]\n",
      "   [-0.58431375 -0.06666667 -0.10588235]\n",
      "   [-0.58431375 -0.05882353 -0.07450981]]]], shape=(24, 128, 128, 3), dtype=float32)\n",
      "[[[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]\n",
      "\n",
      "  [[0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   ...\n",
      "   [0 0 0]\n",
      "   [0 0 0]\n",
      "   [0 0 0]]]]  images is NaN\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-51-3e694ebf9722>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Images_batch: \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimage_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m             \u001B[0mresunetgan\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m             \u001B[0mresunetgan\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msaved_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"mura_data/saved_model/\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/YZU/.jupyter_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1181\u001B[0m                 _r=1):\n\u001B[1;32m   1182\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1183\u001B[0;31m               \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1184\u001B[0m               \u001B[0;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1185\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/YZU/.jupyter_env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    887\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    891\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/YZU/.jupyter_env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    922\u001B[0m       \u001B[0;31m# In this case we have not created variables on the first call. So we can\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    923\u001B[0m       \u001B[0;31m# run the first trace but we should fail if variables are created.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 924\u001B[0;31m       \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    925\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_created_variables\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    926\u001B[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001B[0;32m~/YZU/.jupyter_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3021\u001B[0m       (graph_function,\n\u001B[1;32m   3022\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[0;32m-> 3023\u001B[0;31m     return graph_function._call_flat(\n\u001B[0m\u001B[1;32m   3024\u001B[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[1;32m   3025\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/YZU/.jupyter_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1958\u001B[0m         and executing_eagerly):\n\u001B[1;32m   1959\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1960\u001B[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0m\u001B[1;32m   1961\u001B[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[1;32m   1962\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001B[0;32m~/YZU/.jupyter_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    589\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0m_InterpolateFunctionError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    590\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcancellation_manager\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 591\u001B[0;31m           outputs = execute.execute(\n\u001B[0m\u001B[1;32m    592\u001B[0m               \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msignature\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    593\u001B[0m               \u001B[0mnum_outputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_outputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/YZU/.jupyter_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     57\u001B[0m   \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 59\u001B[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[1;32m     60\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[1;32m     61\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "nSoz_qEh6ZDU"
   },
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()\n",
    "\n",
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F-qJN0EEQwb6"
   },
   "source": [
    "# load an image\n",
    "def load_image_test(filename, size=(128,128)):\n",
    "\t# load image with the preferred size\n",
    "\tpixels = tf.keras.preprocessing.image.load_img(filename, target_size=size)\n",
    "\t# convert to numpy array\n",
    "\tpixels = tf.keras.preprocessing.image.img_to_array(pixels)\n",
    "\t# scale from [0,255] to [-1,1]\n",
    "\tpixels = (pixels - 127.5) / 127.5\n",
    "\t# reshape to 1 sample\n",
    "\tpixels = np.expand_dims(pixels, 0)\n",
    "\treturn pixels"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-8pFOeDNLuR7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "9bfb095a-efef-4a78-e03a-86744b6f168a"
   },
   "source": [
    "# test_images_dataset = tf_dataset(test_images_path, batch_size)\n",
    "# normal_images = glob('mura_data/mura_data/test_data/normal_*.bmp')\n",
    "# defect_images = glob('mura_data/mura_data/test_data/defect_*.bmp')\n",
    "# len_nor_data = len(normal_images)\n",
    "# len_def_data = len(defect_images)\n",
    "# print(len_nor_data)\n",
    "# print(len_def_data)\n",
    "# threshold = 0.6\n",
    "# defect_preds = []\n",
    "# for image in defect_images:\n",
    "#   # print(image)\n",
    "#   if \"DS_Store\" not in image:\n",
    "#     src_image = load_image_test(image)\n",
    "#\n",
    "#     test = d_model.predict(src_image)\n",
    "#     test = (test + 1) / 2.0\n",
    "#     defect_preds = np.append(defect_preds,test)\n",
    "#\n",
    "#     # preds = (preds - preds.min())/(preds.max()-preds.min())\n",
    "#     # print(test)\n",
    "#\n",
    "#\n",
    "#\n",
    "# normal_preds = []\n",
    "# for image in normal_images:\n",
    "#   # print(image)\n",
    "#   if \"DS_Store\" not in image:\n",
    "#     src_image = load_image_test(image)\n",
    "#\n",
    "#     test = d_model.predict(src_image)\n",
    "#     test = (test + 1) / 2.0\n",
    "#     normal_preds = np.append(normal_preds,test)\n",
    "#\n",
    "#     # preds = (preds - preds.min())/(preds.max()-preds.min())\n",
    "#     # print(test)\n",
    "#\n",
    "#\n",
    "# print(defect_preds)\n",
    "# print(np.mean(defect_preds))\n",
    "# true_def_pred = len(np.where(defect_preds > threshold)[0])\n",
    "# print(true_def_pred)\n",
    "#\n",
    "#\n",
    "# print(normal_preds)\n",
    "# print(np.mean(normal_preds))\n",
    "# true_nor_pred = len(np.where(normal_preds < threshold)[0])\n",
    "# print(true_nor_pred)\n",
    "#\n",
    "# total_acc = (true_def_pred + true_nor_pred) / (len_nor_data + len_def_data) * 100\n",
    "# print(\"total_accuracy: \", total_acc)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "51\n",
      "50\n",
      "[0.64748341 0.64582115 0.64495695 0.63706183 0.64389598 0.64372158\n",
      " 0.64029634 0.64423901 0.63870716 0.64056474 0.63585079 0.63704026\n",
      " 0.6411283  0.64476216 0.63866848 0.63880849 0.63715732 0.63976002\n",
      " 0.64145702 0.64556265 0.64365792 0.6378026  0.64362186 0.63863498\n",
      " 0.64006096 0.64166403 0.64609551 0.63979173 0.64554846 0.63616109\n",
      " 0.64451241 0.64017868 0.6469053  0.63884485 0.63761288 0.64289093\n",
      " 0.64132708 0.64631164 0.64095742 0.63797343 0.64088923 0.63647813\n",
      " 0.65117645 0.63967681 0.64133358 0.63783252 0.64289385 0.64198399\n",
      " 0.63981831 0.63435316]\n",
      "0.6412786686420441\n",
      "50\n",
      "[0.63939631 0.64166212 0.63711405 0.63674265 0.63656467 0.63669407\n",
      " 0.63940752 0.63682628 0.63999265 0.63952291 0.63734055 0.64020377\n",
      " 0.63860971 0.640733   0.64046967 0.63901877 0.63815737 0.63886672\n",
      " 0.6363433  0.63823372 0.63949871 0.63427675 0.63712913 0.64116955\n",
      " 0.63772613 0.63943613 0.63968712 0.6397562  0.63884401 0.64015615\n",
      " 0.63704967 0.64029896 0.63756126 0.63938802 0.63810068 0.63947988\n",
      " 0.63981718 0.6367234  0.63923091 0.63839173 0.63806874 0.63837016\n",
      " 0.64042914 0.64004749 0.6408875  0.63947093 0.63853627 0.63996935\n",
      " 0.63839281 0.63756096]\n",
      "0.6387470948696137\n",
      "0\n",
      "total_accuracy:  49.504950495049506\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "x3EaS4Uz6ZDV"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}