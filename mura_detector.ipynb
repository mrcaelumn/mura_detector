{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "mura_detector.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import PIL\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "IMG_H = 128\n",
    "IMG_W = 128\n",
    "IMG_C = 3  ## Change this to 1 for grayscale.\n",
    "\n",
    "\n",
    "# Regularization Rate for each loss function\n",
    "ADV_REG_RATE_LF = 1\n",
    "REC_REG_RATE_LF = 50\n",
    "SSIM_REG_RATE_LF = 50\n",
    "FEAT_REG_RATE_LF = 1\n",
    "\n",
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Loss function for evaluating adversarial loss\n",
    "adv_loss_fn = tf.losses.MeanSquaredError()\n",
    "\n",
    "w_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QjzYjrZEp3k9"
   },
   "source": [
    "def load_image(image_path):\n",
    "  img = tf.io.read_file(image_path)\n",
    "  img = tf.io.decode_bmp(img)\n",
    "  img = tf.image.resize_with_crop_or_pad(img, IMG_H, IMG_W)\n",
    "  img = tf.cast(img, tf.float32)\n",
    "  img = (img - 127.5) / 127.5\n",
    "  return img"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QT7glm2zsyYk"
   },
   "source": [
    "def tf_dataset(images_path, batch_size):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(images_path)\n",
    "  dataset = dataset.shuffle(buffer_size=10240)\n",
    "  dataset = dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "  return dataset"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xFL24bEX65GT"
   },
   "source": [
    "def conv_block(input, num_filters):\n",
    "    x = tf.keras.layers.Conv2D(num_filters, kernel_size=(1,1), padding=\"same\")(input)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(num_filters, kernel_size=(3,3), padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "    return x"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aPVAOjNg69AQ"
   },
   "source": [
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = tf.keras.layers.Conv2DTranspose(num_filters, (1, 1), strides=2, padding=\"same\")(input)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1rZBRUwR2GGx"
   },
   "source": [
    "# loss function for SSIM\n",
    "def SSIMLoss(x_actual, x_recon):\n",
    "  return 1 - tf.reduce_mean(tf.image.ssim(x_actual, x_recon, 1.0))\n",
    "\n",
    "# loss functio for adversial\n",
    "def ADVLoss(x_actual, x_recon):\n",
    "  loss_act = tf.losses.MeanSquaredError(tf.ones_like(x_actual), x_actual)\n",
    "  loss_rec = tf.losses.MeanSquaredError(tf.zeros_like(x_recon), x_recon)\n",
    "  return tf.losses.mae(loss_act, loss_rec)\n",
    "\n",
    "# loss function for feature\n",
    "def FEATLoss(x_actual, x_recon):\n",
    "  return tf.keras.losses.mean_absolute_error(x_actual, x_recon)\n",
    "\n",
    "# loss function for reconstruction\n",
    "def RECONLoss(x_actual, x_recon):\n",
    "  print(\"recon\")\n",
    "  return tf.keras.losses.mae(x_recon, x_actual)\n",
    "  \n"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "svLeaykpwng2"
   },
   "source": [
    "def gen_loss_func(x_actual, x_recon):\n",
    "  #test\n",
    "  # print(\"custom loss function for generator\")\n",
    "  return (SSIM_REG_RATE_LF * SSIMLoss(x_actual, x_recon)) + (ADV_REG_RATE_LF * ADVLoss(x_actual, x_recon)) + (FEAT_REG_RATE_LF * FEATLoss(x_actual, x_recon)) + (REC_REG_RATE_LF * RECONLoss(x_recon, x_actual))\n",
    "\n",
    "\n",
    "def generator_loss(x_recon):\n",
    "  # return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "  # recon_loss = adv_loss_fn(tf.ones_like(x_recon), x_recon)\n",
    "  # return recon_loss * ADV_REG_RATE_LF\n",
    "  # return recon_loss\n",
    "  return -tf.reduce_mean(tf.math.log(x_recon + 1e-10))"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vFebXNsAxKqN"
   },
   "source": [
    "def disc_loss_func(x_actual, x_recon):\n",
    "  #test\n",
    "  # print(\"custom loss function for discrimnator\")\n",
    "  return (ADV_REG_RATE_LF * ADVLoss(x_actual, x_recon)) + (FEAT_REG_RATE_LF * FEATLoss(x_actual, x_recon))\n",
    "\n",
    "def discriminator_loss(x_actual, x_recon):\n",
    "  # act_loss = adv_loss_fn(tf.ones_like(x_actual), x_actual)\n",
    "  # recon_loss = adv_loss_fn(tf.zeros_like(x_recon), x_recon)\n",
    "  # # return (act_loss + recon_loss) * ADV_REG_RATE_LF\n",
    "  # return (act_loss + recon_loss) * 0.5\n",
    "  return -tf.reduce_mean(tf.math.log(x_actual + 1e-10) + tf.math.log(1. - x_recon + 1e-10))\n"
   ],
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bm_XokrmFnlN"
   },
   "source": [
    "class ResUnetGAN(tf.keras.models.Model):\n",
    "    def __init__(self, input_shape, batch_size):\n",
    "        super(ResUnetGAN, self).__init__()\n",
    "        self.discriminator = self.build_discriminator(input_shape)\n",
    "        self.generator = self.build_generator_resnet50_unet(input_shape)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        print(\"test\")\n",
    "\n",
    "\n",
    "        # self.discriminator.summary()\n",
    "        # self.generator.summary()\n",
    "\n",
    "    # create generator model based on resnet50 and unet network\n",
    "    def build_generator_resnet50_unet(self, input_shape):\n",
    "        # print(inputs)\n",
    "        # print(\"pretained start\")\n",
    "        \"\"\" Pre-trained ResNet50 Model \"\"\"\n",
    "        resnet50 = tf.keras.applications.ResNet50(include_top=False, weights=\"imagenet\", input_tensor=input_shape)\n",
    "        # print(\"testing\")\n",
    "        \"\"\" Encoder using resnet50\"\"\"\n",
    "        # for layer in resnet50.layers:\n",
    "        #   print(layer.name)\n",
    "        s1 = resnet50.get_layer(\"input_1\").output           ## (128 x 128)\n",
    "        # print(s1)\n",
    "        s2 = resnet50.get_layer(\"conv1_relu\").output        ## (64 x 64)\n",
    "        s3 = resnet50.get_layer(\"conv2_block3_out\").output  ## (32 x 32)\n",
    "        s4 = resnet50.get_layer(\"conv3_block4_out\").output  ## (16 x 16)\n",
    "\n",
    "        \"\"\" Bridge \"\"\"\n",
    "        b1 = resnet50.get_layer(\"conv4_block6_out\").output  ## (32 x 32)\n",
    "\n",
    "        # print(\"test\")\n",
    "        # print(b1.get_weights())\n",
    "        \"\"\" Decoder unet\"\"\"\n",
    "        d1 = decoder_block(b1, s4, 128)                     ## (16 x 16)\n",
    "        d2 = decoder_block(d1, s3, 64)                     ## (32 x 32)\n",
    "        d3 = decoder_block(d2, s2, 32)                     ## (64 x 64)\n",
    "        d4 = decoder_block(d3, s1, 16)                      ## (128 x 128)\n",
    "\n",
    "        \"\"\" Output \"\"\"\n",
    "        outputs = tf.keras.layers.Conv2D(3, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs, outputs)\n",
    "\n",
    "        return model\n",
    "\n",
    "    # create discriminator model\n",
    "\n",
    "    def build_discriminator(self ,input_shape):\n",
    "      # Load the pre-trained model and freeze it.\n",
    "\n",
    "\n",
    "        x = tf.keras.layers.SeparableConvolution2D(32,kernel_size= (1, 1), strides=(2, 2), padding='same')(input_shape)\n",
    "        x = tf.keras.layers.LeakyReLU()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "        x = tf.keras.layers.SeparableConvolution2D(64,kernel_size=(1, 1), strides=(2, 2), padding='same')(x)\n",
    "        x = tf.keras.layers.LeakyReLU()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs, x)\n",
    "        return model\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, gen_loss_fn, disc_loss_fn):\n",
    "        super(ResUnetGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.gen_loss_fn = gen_loss_fn\n",
    "        self.disc_loss_fn = disc_loss_fn\n",
    "  \n",
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "    @tf.function\n",
    "    def train_step(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        # print(batch_size, IMG_W, IMG_H, IMG_C)\n",
    "        # print(\"test\")\n",
    "        print(self.generator)\n",
    "        print(self.discriminator)\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "          generated_images = self.generator(images, training=True)\n",
    "          real_output = self.discriminator(images, training=True)\n",
    "          # print(generated_images.shape)\n",
    "          fake_output = self.discriminator(generated_images, training=True)\n",
    "          gen_loss = self.gen_loss_fn(fake_output)\n",
    "          disc_loss = self.disc_loss_fn(real_output, fake_output)\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "        self.g_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "        self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "\n",
    "        return {\"gen_loss\": gen_loss, \"disc_loss\": disc_loss}\n",
    "\n",
    "    def saved_model(self, filepath, num_of_epoch):\n",
    "        self.generator.save(filepath + \"g_model\" + num_of_epoch + \".h5\")\n",
    "        self.discriminator.save(filepath + \"d_model\" + num_of_epoch + \".h5\")\n",
    "\n",
    "    def loaded_model(self, filepath):\n",
    "        self.generator.load_weights(filepath)\n",
    "        self.discriminator.load_weights(filepath)"
   ],
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KoSI9-4-tVt",
    "outputId": "f70651c0-9382-43de-ac20-6bb2475741ac"
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # run the function here\n",
    "    print(\"start\")\n",
    "    ## Hyperparameters\n",
    "    batch_size = 24\n",
    "    input_shape = (IMG_W, IMG_H, IMG_C)\n",
    "    # print(input_shape)\n",
    "\n",
    "    \"\"\" Input \"\"\"\n",
    "    inputs = tf.keras.layers.Input(input_shape, name=\"input_1\")\n",
    "\n",
    "    num_epochs = 2\n",
    "    train_images_path = glob(\"mura_data/mura_data/train_data/*\")\n",
    "\n",
    "\n",
    "    # d_model = build_discriminator(inputs)\n",
    "    # g_model = build_generator_resnet50_unet(inputs)\n",
    "\n",
    "    resunetgan = ResUnetGAN(inputs, batch_size)\n",
    "\n",
    "\n",
    "    g_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5, beta_2=0.999)\n",
    "    d_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5, beta_2=0.999)\n",
    "    resunetgan.compile(d_optimizer, g_optimizer, generator_loss, discriminator_loss)\n",
    "\n",
    "    # print(train_images_path)\n",
    "    train_images_dataset = tf_dataset(train_images_path, batch_size)\n",
    "\n",
    "    # resunetgan.fit(train_images_dataset)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: \", epoch)\n",
    "        start = time.time()\n",
    "        for image_batch in train_images_dataset:\n",
    "        # print(image_batch.shape)\n",
    "            resunetgan.fit(image_batch)\n",
    "            resunetgan.saved_model(\"saved_model/\", num_epochs)\n",
    "\n",
    "    # resunetgan.summary()\n",
    "    # resunetgan.save_weights(\"saved_model/resunet_model\")\n"
   ],
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "test\n",
      "Epoch:  0\n",
      "<tensorflow.python.keras.engine.functional.Functional object at 0x165d14340>\n",
      "<tensorflow.python.keras.engine.functional.Functional object at 0x165579880>\n",
      "<tensorflow.python.keras.engine.functional.Functional object at 0x165d14340>\n",
      "<tensorflow.python.keras.engine.functional.Functional object at 0x165579880>\n",
      "1/1 [==============================] - 13s 13s/step - gen_loss: 0.9967 - disc_loss: 0.4970\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1/1 [==============================] - 2s 2s/step - gen_loss: 0.9933 - disc_loss: 0.4637\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1/1 [==============================] - 2s 2s/step - gen_loss: 0.9962 - disc_loss: 0.4327\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1/1 [==============================] - 3s 3s/step - gen_loss: 0.9935 - disc_loss: 0.3939\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1/1 [==============================] - 2s 2s/step - gen_loss: 0.9880 - disc_loss: 0.3589\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1/1 [==============================] - 2s 2s/step - gen_loss: 0.9641 - disc_loss: 0.3145\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1/1 [==============================] - 1s 673ms/step - gen_loss: 0.9344 - disc_loss: 0.2698\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch:  1\n",
      "1/1 [==============================] - 3s 3s/step - gen_loss: 0.9100 - disc_loss: 0.2279\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1/1 [==============================] - 3s 3s/step - gen_loss: 0.8707 - disc_loss: 0.1836\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1/1 [==============================] - 3s 3s/step - gen_loss: 0.8272 - disc_loss: 0.1456\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1/1 [==============================] - 2s 2s/step - gen_loss: 0.7856 - disc_loss: 0.1085\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1/1 [==============================] - 2s 2s/step - gen_loss: 0.7509 - disc_loss: 0.0805\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1/1 [==============================] - 3s 3s/step - gen_loss: 0.7231 - disc_loss: 0.0575\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1/1 [==============================] - 1s 685ms/step - gen_loss: 0.7040 - disc_loss: 0.0455\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()\n",
    "\n",
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F-qJN0EEQwb6"
   },
   "source": [
    "# load an image\n",
    "def load_image_test(filename, size=(128,128)):\n",
    "\t# load image with the preferred size\n",
    "\tpixels = tf.keras.preprocessing.image.load_img(filename, target_size=size)\n",
    "\t# convert to numpy array\n",
    "\tpixels = tf.keras.preprocessing.image.img_to_array(pixels)\n",
    "\t# scale from [0,255] to [-1,1]\n",
    "\tpixels = (pixels - 127.5) / 127.5\n",
    "\t# reshape to 1 sample\n",
    "\tpixels = np.expand_dims(pixels, 0)\n",
    "\treturn pixels"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-8pFOeDNLuR7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9bfb095a-efef-4a78-e03a-86744b6f168a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# test_images_dataset = tf_dataset(test_images_path, batch_size)\n",
    "# normal_images = glob('mura_data/mura_data/test_data/normal_*.bmp')\n",
    "# defect_images = glob('mura_data/mura_data/test_data/defect_*.bmp')\n",
    "# len_nor_data = len(normal_images)\n",
    "# len_def_data = len(defect_images)\n",
    "# print(len_nor_data)\n",
    "# print(len_def_data)\n",
    "# threshold = 0.6\n",
    "# defect_preds = []\n",
    "# for image in defect_images:\n",
    "#   # print(image)\n",
    "#   if \"DS_Store\" not in image:\n",
    "#     src_image = load_image_test(image)\n",
    "#\n",
    "#     test = d_model.predict(src_image)\n",
    "#     test = (test + 1) / 2.0\n",
    "#     defect_preds = np.append(defect_preds,test)\n",
    "#\n",
    "#     # preds = (preds - preds.min())/(preds.max()-preds.min())\n",
    "#     # print(test)\n",
    "#\n",
    "#\n",
    "#\n",
    "# normal_preds = []\n",
    "# for image in normal_images:\n",
    "#   # print(image)\n",
    "#   if \"DS_Store\" not in image:\n",
    "#     src_image = load_image_test(image)\n",
    "#\n",
    "#     test = d_model.predict(src_image)\n",
    "#     test = (test + 1) / 2.0\n",
    "#     normal_preds = np.append(normal_preds,test)\n",
    "#\n",
    "#     # preds = (preds - preds.min())/(preds.max()-preds.min())\n",
    "#     # print(test)\n",
    "#\n",
    "#\n",
    "# print(defect_preds)\n",
    "# print(np.mean(defect_preds))\n",
    "# true_def_pred = len(np.where(defect_preds > threshold)[0])\n",
    "# print(true_def_pred)\n",
    "#\n",
    "#\n",
    "# print(normal_preds)\n",
    "# print(np.mean(normal_preds))\n",
    "# true_nor_pred = len(np.where(normal_preds < threshold)[0])\n",
    "# print(true_nor_pred)\n",
    "#\n",
    "# total_acc = (true_def_pred + true_nor_pred) / (len_nor_data + len_def_data) * 100\n",
    "# print(\"total_accuracy: \", total_acc)\n",
    "\n"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "50\n",
      "[0.64748341 0.64582115 0.64495695 0.63706183 0.64389598 0.64372158\n",
      " 0.64029634 0.64423901 0.63870716 0.64056474 0.63585079 0.63704026\n",
      " 0.6411283  0.64476216 0.63866848 0.63880849 0.63715732 0.63976002\n",
      " 0.64145702 0.64556265 0.64365792 0.6378026  0.64362186 0.63863498\n",
      " 0.64006096 0.64166403 0.64609551 0.63979173 0.64554846 0.63616109\n",
      " 0.64451241 0.64017868 0.6469053  0.63884485 0.63761288 0.64289093\n",
      " 0.64132708 0.64631164 0.64095742 0.63797343 0.64088923 0.63647813\n",
      " 0.65117645 0.63967681 0.64133358 0.63783252 0.64289385 0.64198399\n",
      " 0.63981831 0.63435316]\n",
      "0.6412786686420441\n",
      "50\n",
      "[0.63939631 0.64166212 0.63711405 0.63674265 0.63656467 0.63669407\n",
      " 0.63940752 0.63682628 0.63999265 0.63952291 0.63734055 0.64020377\n",
      " 0.63860971 0.640733   0.64046967 0.63901877 0.63815737 0.63886672\n",
      " 0.6363433  0.63823372 0.63949871 0.63427675 0.63712913 0.64116955\n",
      " 0.63772613 0.63943613 0.63968712 0.6397562  0.63884401 0.64015615\n",
      " 0.63704967 0.64029896 0.63756126 0.63938802 0.63810068 0.63947988\n",
      " 0.63981718 0.6367234  0.63923091 0.63839173 0.63806874 0.63837016\n",
      " 0.64042914 0.64004749 0.6408875  0.63947093 0.63853627 0.63996935\n",
      " 0.63839281 0.63756096]\n",
      "0.6387470948696137\n",
      "0\n",
      "total_accuracy:  49.504950495049506\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}