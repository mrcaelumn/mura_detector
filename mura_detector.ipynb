{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "mura_detector.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ISLP7e8o6ZDK"
   },
   "source": [
    "import PIL\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import time\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "IMG_H = 128\n",
    "IMG_W = 128\n",
    "IMG_C = 3  ## Change this to 1 for grayscale.\n",
    "\n",
    "\n",
    "# Regularization Rate for each loss function\n",
    "\n",
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Loss function for evaluating adversarial loss\n",
    "adv_loss_fn = tf.losses.MeanSquaredError()\n",
    "\n",
    "w_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QjzYjrZEp3k9"
   },
   "source": [
    "def load_image(image_path):\n",
    "  img = tf.io.read_file(image_path)\n",
    "  img = tf.io.decode_bmp(img)\n",
    "  img = tf.image.resize_with_crop_or_pad(img, IMG_H, IMG_W)\n",
    "  img = tf.cast(img, tf.float32)\n",
    "  img = (img - 127.5) / 127.5\n",
    "  return img"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QT7glm2zsyYk"
   },
   "source": [
    "def tf_dataset(images_path, batch_size):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(images_path)\n",
    "  dataset = dataset.shuffle(buffer_size=10240)\n",
    "  dataset = dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "  return dataset"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xFL24bEX65GT"
   },
   "source": [
    "def bn_act(x, act=True):\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    if act:\n",
    "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "def conv_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    conv = bn_act(x)\n",
    "    conv = tf.keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(conv)\n",
    "    return conv\n",
    "\n",
    "def stem(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    conv = tf.keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n",
    "    conv = conv_block(conv, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
    "\n",
    "    shortcut = tf.keras.layers.Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
    "    shortcut = bn_act(shortcut, act=False)\n",
    "\n",
    "    output = tf.keras.layers.Add()([conv, shortcut])\n",
    "    return output\n",
    "\n",
    "def residual_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
    "    res = conv_block(x, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
    "    res = conv_block(res, filters, kernel_size=kernel_size, padding=padding, strides=1)\n",
    "\n",
    "    shortcut = tf.keras.layers.Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
    "    shortcut = bn_act(shortcut, act=False)\n",
    "\n",
    "    output = tf.keras.layers.Add()([shortcut, res])\n",
    "    return output\n",
    "\n",
    "def upsample_concat_block(x, xskip):\n",
    "    u = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "    c = tf.keras.layers.Concatenate()([u, xskip])"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bm_XokrmFnlN"
   },
   "source": [
    "class ResUnetGAN(tf.keras.models.Model):\n",
    "    def __init__(self, input_shape, batch_size):\n",
    "        super(ResUnetGAN, self).__init__()\n",
    "        self.discriminator = self.build_discriminator(input_shape)\n",
    "        self.generator = self.build_generator_resnet50_unet(input_shape)\n",
    "        self.batch_size = batch_size\n",
    "        self.ADV_REG_RATE_LF = 1\n",
    "        self.REC_REG_RATE_LF = 50\n",
    "        self.SSIM_REG_RATE_LF = 50\n",
    "        self.FEAT_REG_RATE_LF = 1\n",
    "        self.d_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5, beta_2=0.999)\n",
    "        self.g_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "        checkpoint_dir = './training_checkpoints'\n",
    "        self.checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "        self.checkpoint = tf.train.Checkpoint(generator_optimizer=self.g_optimizer,\n",
    "                                 discriminator_optimizer=self.d_optimizer,\n",
    "                                 generator=self.generator,\n",
    "                                 discriminator=self.discriminator)\n",
    "        # self.discriminator.summary()\n",
    "        # self.generator.summary()\n",
    "\n",
    "    # create generator model based on resnet50 and unet network\n",
    "    def build_generator_resnet50_unet(self, input_shape):\n",
    "        f = [16, 32, 64, 128, 256]\n",
    "        inputs = tf.keras.layers.Input(input_shape)\n",
    "\n",
    "        ## Encoder\n",
    "        e0 = inputs\n",
    "        e1 = stem(e0, f[0])\n",
    "        e2 = residual_block(e1, f[1], strides=2)\n",
    "        e3 = residual_block(e2, f[2], strides=2)\n",
    "        e4 = residual_block(e3, f[3], strides=2)\n",
    "        e5 = residual_block(e4, f[4], strides=2)\n",
    "\n",
    "        ## Bridge\n",
    "        b0 = conv_block(e5, f[4], strides=1)\n",
    "        b1 = conv_block(b0, f[4], strides=1)\n",
    "\n",
    "        ## Decoder\n",
    "        u1 = upsample_concat_block(b1, e4)\n",
    "        d1 = residual_block(u1, f[4])\n",
    "\n",
    "        u2 = upsample_concat_block(d1, e3)\n",
    "        d2 = residual_block(u2, f[3])\n",
    "\n",
    "        u3 = upsample_concat_block(d2, e2)\n",
    "        d3 = residual_block(u3, f[2])\n",
    "\n",
    "        u4 = upsample_concat_block(d3, e1)\n",
    "        d4 = residual_block(u4, f[1])\n",
    "\n",
    "        outputs = tf.keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "        model = tf.keras.models.Model(inputs, outputs)\n",
    "\n",
    "        return model\n",
    "\n",
    "    # create discriminator model\n",
    "\n",
    "    def build_discriminator(self ,input_shape):\n",
    "\n",
    "        x = tf.keras.layers.SeparableConvolution2D(32,kernel_size= (1, 1), strides=(2, 2), padding='same')(input_shape)\n",
    "        x = tf.keras.layers.LeakyReLU()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "        x = tf.keras.layers.SeparableConvolution2D(64,kernel_size=(1, 1), strides=(2, 2), padding='same')(x)\n",
    "        x = tf.keras.layers.LeakyReLU()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs, x)\n",
    "        return model\n",
    "        # return x\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer):\n",
    "        super(ResUnetGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "\n",
    "  \n",
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "    @tf.function\n",
    "    def train_step(self, images):\n",
    "\n",
    "\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            # tf.print(\"Images: \", images)\n",
    "            reconstructed_images, low_feature = self.generator(images, training=True)\n",
    "            real_output = self.discriminator(images, training=True)\n",
    "            # print(generated_images.shape)\n",
    "            fake_output = self.discriminator(reconstructed_images, training=True)\n",
    "            \n",
    "            # if tf.math.is_nan(real_output) is not None:\n",
    "            #     tf.print(tf.math.is_nan(real_output),\" real_output is NaN: \", real_output)\n",
    "            # # else:\n",
    "            #     # tf.print(real_output)\n",
    "            # #\n",
    "            # if tf.math.is_nan(fake_output) is not None:\n",
    "            #     tf.print(tf.math.is_nan(fake_output),\" fake_output is NaN: \", fake_output)\n",
    "            # # else:\n",
    "            # #     tf.print(fake_output)\n",
    "            # #\n",
    "            # if tf.math.is_nan(reconstructed_images) is not None:\n",
    "            #     tf.print(tf.math.is_nan(reconstructed_images),\" reconstructed_images is NaN: \",reconstructed_images)\n",
    "            # else:\n",
    "            #     tf.print(reconstructed_images)\n",
    "\n",
    "            # Loss 1: ADVERSARIAL loss\n",
    "            loss_adv = tf.reduce_mean(tf.math.log(real_output) + tf.math.log(1 - fake_output),keepdims=True)\n",
    "            # Loss 2: RECONSTRUCTION loss\n",
    "            loss_rec = tf.math.reduce_sum(tf.math.abs(images - reconstructed_images), keepdims=True)\n",
    "            # Loss 3: SSIM loss\n",
    "            loss_ssim = tf.math.reduce_sum(1 - tf.image.ssim(images,reconstructed_images, max_val=1.0)[0], keepdims=True)\n",
    "            # Loss 4: FEATURE loss\n",
    "            loss_feat = tf.math.reduce_sum(tf.math.square(real_output - fake_output), keepdims=True)\n",
    "\n",
    "            gen_loss = tf.math.reduce_mean((loss_adv * self.ADV_REG_RATE_LF) + (loss_rec * self.REC_REG_RATE_LF) + (loss_ssim * self.SSIM_REG_RATE_LF) + (loss_feat * self.FEAT_REG_RATE_LF))\n",
    "            disc_loss = tf.math.reduce_mean((loss_adv * self.ADV_REG_RATE_LF) + (loss_feat * self.FEAT_REG_RATE_LF))\n",
    "\n",
    "        \n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "        \n",
    "        self.g_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "        self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "\n",
    "        # self.mean_loss_adv = tf.math.reduce_mean(loss_adv)\n",
    "        # self.mean_loss_rec = tf.math.reduce_mean(loss_rec)\n",
    "        # self.mean_loss_ssim = tf.math.reduce_mean(loss_ssim)\n",
    "        # self.mean_loss_feat = tf.math.reduce_mean(loss_feat)\n",
    "\n",
    "        # tf.summary.scalar('loss_adv', self.mean_loss_adv)\n",
    "        # tf.summary.scalar('loss_rec', self.mean_loss_rec)\n",
    "        # tf.summary.scalar('loss_ssim', self.mean_loss_ssim)\n",
    "        # tf.summary.scalar('loss_feat', self.mean_loss_feat)\n",
    "        # tf.summary.scalar('gen_loss', gen_loss)\n",
    "        # tf.summary.scalar('disc_loss', disc_loss)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"gen_loss\": gen_loss,\n",
    "            \"disc_loss\": disc_loss,\n",
    "            \"loss_adv\": loss_adv,\n",
    "            \"loss_rec\": loss_rec,\n",
    "            \"loss_ssim\": loss_ssim,\n",
    "            \"loss_feat\": loss_feat\n",
    "        }\n",
    "\n",
    "    def saved_model(self, filepath, num_of_epoch):\n",
    "        self.generator.save(filepath + \"g_model\" + str(num_of_epoch) + \".h5\")\n",
    "        self.discriminator.save(filepath + \"d_model\" + str(num_of_epoch) + \".h5\")\n",
    "\n",
    "    def loaded_model(self, filepath):\n",
    "        self.generator.load_weights(filepath)\n",
    "        self.discriminator.load_weights(filepath)\n",
    "\n",
    "\n",
    "\n",
    "    # def train(self, dataset, epochs):\n",
    "    #     for epoch in range(epochs):\n",
    "    #         start = time.time()\n",
    "    #         print(\"Epoch: \",)\n",
    "    #         for image_batch in dataset:\n",
    "    #             # print(image_batch)\n",
    "    #             output = self.train_step(image_batch)\n",
    "    #             # tf.print(output)\n",
    "\n",
    "    #         # Produce images for the GIF as you go\n",
    "    #         # display.clear_output(wait=True)\n",
    "    #         # generate_and_save_images(generator,\n",
    "    #         #                      epoch + 1,\n",
    "    #         #                      seed)\n",
    "    #         # Save the model every 15 epochs\n",
    "    #         if (epoch + 1) % 15 == 0:\n",
    "    #             self.checkpoint.save(file_prefix = self.checkpoint_prefix)\n",
    "    #             self.saved_model(\"/content/drive/MyDrive/mura_data/saved_model/\", num_epochs)\n",
    "    #             print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KoSI9-4-tVt",
    "outputId": "a537c654-02e1-4c8e-9aaf-034d3e44ba82"
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # run the function here\n",
    "    print(\"start\")\n",
    "    ## Hyperparameters\n",
    "    batch_size = 24\n",
    "    input_shape = (IMG_W, IMG_H, IMG_C)\n",
    "    # print(input_shape)\n",
    "\n",
    "    \"\"\" Input \"\"\"\n",
    "    inputs = tf.keras.layers.Input(input_shape, name=\"input_1\")\n",
    "\n",
    "    num_epochs = 600\n",
    "    train_images_path = glob(\"/content/drive/MyDrive/mura_data/train_data/*.bmp\")\n",
    "\n",
    "\n",
    "    # d_model = build_discriminator(inputs)\n",
    "    # g_model = build_generator_resnet50_unet(inputs)\n",
    "\n",
    "    resunetgan = ResUnetGAN(inputs, batch_size)\n",
    "\n",
    "\n",
    "    g_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5, beta_2=0.999)\n",
    "    d_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5, beta_2=0.999)\n",
    "    resunetgan.compile(d_optimizer, g_optimizer)\n",
    "\n",
    "    # print(train_images_path)\n",
    "    train_images_dataset = tf_dataset(train_images_path, batch_size)\n",
    "\n",
    "    # resunetgan.fit(train_images_dataset)\n",
    "\n",
    "    # resunetgan.train(train_images_dataset, num_epochs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: \", epoch)\n",
    "        start = time.time()\n",
    "        for image_batch in train_images_dataset:\n",
    "        # print(image_batch.shape)\n",
    "            # print(\"Images_batch: \", image_batch)\n",
    "            resunetgan.fit(image_batch)\n",
    "            resunetgan.saved_model(\"/content/drive/MyDrive/mura_data/saved_model/\", num_epochs)\n",
    "\n",
    "    # resunetgan.summary()\n",
    "    # resunetgan.save_weights(\"saved_model/resunet_model\")\n",
    "\n"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F-qJN0EEQwb6"
   },
   "source": [
    "# load an image\n",
    "def load_image_test(filename, size=(128,128)):\n",
    "\t# load image with the preferred size\n",
    "\tpixels = tf.keras.preprocessing.image.load_img(filename, target_size=size)\n",
    "\t# convert to numpy array\n",
    "\tpixels = tf.keras.preprocessing.image.img_to_array(pixels)\n",
    "\t# scale from [0,255] to [-1,1]\n",
    "\tpixels = (pixels - 127.5) / 127.5\n",
    "\t# reshape to 1 sample\n",
    "\tpixels = np.expand_dims(pixels, 0)\n",
    "\treturn pixels"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Found 100 files belonging to 2 classes.\n",
      "<BatchDataset shapes: ((None, 128, 128, 3), (None,)), types: (tf.float32, tf.int32)>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot assign to variable conv2d_transpose_20/kernel:0 due to variable shape (2, 2, 128, 1024) and value shape (1024, 128, 1, 1) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-22-973b4ea4baa0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     31\u001B[0m     \u001B[0;31m# resunetgan.train(train_images_dataset, num_epochs)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 33\u001B[0;31m     resunetgan.test_and_eval(\"mura_data/mura_data/test_data/\",\n\u001B[0m\u001B[1;32m     34\u001B[0m                              \u001B[0;34m\"mura_data/saved_model/g_model_500.h5\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m                              \"mura_data/saved_model/d_model_500.h5\")\n",
      "\u001B[0;32m<ipython-input-21-40a8a581d0f5>\u001B[0m in \u001B[0;36mtest_and_eval\u001B[0;34m(self, filepath, g_filepath, d_filepath)\u001B[0m\n\u001B[1;32m    163\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_dateset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    164\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 165\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloaded_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mg_filepath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0md_filepath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    166\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-21-40a8a581d0f5>\u001B[0m in \u001B[0;36mloaded_model\u001B[0;34m(self, g_filepath, d_filepath)\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    156\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mloaded_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mg_filepath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0md_filepath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 157\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_weights\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mg_filepath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    158\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdiscriminator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_weights\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md_filepath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    159\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/YZU/.jupyter_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mload_weights\u001B[0;34m(self, filepath, by_name, skip_mismatch, options)\u001B[0m\n\u001B[1;32m   2324\u001B[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001B[1;32m   2325\u001B[0m       \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2326\u001B[0;31m         \u001B[0mhdf5_format\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_weights_from_hdf5_group\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2327\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2328\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_updated_config\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/YZU/.jupyter_env/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001B[0m in \u001B[0;36mload_weights_from_hdf5_group\u001B[0;34m(f, layers)\u001B[0m\n\u001B[1;32m    711\u001B[0m                        str(len(weight_values)) + ' elements.')\n\u001B[1;32m    712\u001B[0m     \u001B[0mweight_value_tuples\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msymbolic_weights\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight_values\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 713\u001B[0;31m   \u001B[0mbackend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_set_value\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mweight_value_tuples\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    714\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    715\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/YZU/.jupyter_env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    204\u001B[0m     \u001B[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    205\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 206\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    207\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    208\u001B[0m       \u001B[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/YZU/.jupyter_env/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\u001B[0m in \u001B[0;36mbatch_set_value\u001B[0;34m(tuples)\u001B[0m\n\u001B[1;32m   3802\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecuting_eagerly_outside_functions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3803\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtuples\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3804\u001B[0;31m       \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0massign\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3805\u001B[0m   \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3806\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mget_graph\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mas_default\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/YZU/.jupyter_env/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001B[0m in \u001B[0;36massign\u001B[0;34m(self, value, use_locking, name, read_value)\u001B[0m\n\u001B[1;32m    896\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    897\u001B[0m           \u001B[0mtensor_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\" \"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 898\u001B[0;31m         raise ValueError(\n\u001B[0m\u001B[1;32m    899\u001B[0m             (\"Cannot assign to variable%s due to variable shape %s and value \"\n\u001B[1;32m    900\u001B[0m              \"shape %s are incompatible\") %\n",
      "\u001B[0;31mValueError\u001B[0m: Cannot assign to variable conv2d_transpose_20/kernel:0 due to variable shape (2, 2, 128, 1024) and value shape (1024, 128, 1, 1) are incompatible"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-8pFOeDNLuR7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "9bfb095a-efef-4a78-e03a-86744b6f168a"
   },
   "source": [
    "# test_images_dataset = tf_dataset(test_images_path, batch_size)\n",
    "# normal_images = glob('mura_data/mura_data/test_data/normal_*.bmp')\n",
    "# defect_images = glob('mura_data/mura_data/test_data/defect_*.bmp')\n",
    "# len_nor_data = len(normal_images)\n",
    "# len_def_data = len(defect_images)\n",
    "# print(len_nor_data)\n",
    "# print(len_def_data)\n",
    "# threshold = 0.6\n",
    "# defect_preds = []\n",
    "# for image in defect_images:\n",
    "#   # print(image)\n",
    "#   if \"DS_Store\" not in image:\n",
    "#     src_image = load_image_test(image)\n",
    "#\n",
    "#     test = d_model.predict(src_image)\n",
    "#     test = (test + 1) / 2.0\n",
    "#     defect_preds = np.append(defect_preds,test)\n",
    "#\n",
    "#     # preds = (preds - preds.min())/(preds.max()-preds.min())\n",
    "#     # print(test)\n",
    "#\n",
    "#\n",
    "#\n",
    "# normal_preds = []\n",
    "# for image in normal_images:\n",
    "#   # print(image)\n",
    "#   if \"DS_Store\" not in image:\n",
    "#     src_image = load_image_test(image)\n",
    "#\n",
    "#     test = d_model.predict(src_image)\n",
    "#     test = (test + 1) / 2.0\n",
    "#     normal_preds = np.append(normal_preds,test)\n",
    "#\n",
    "#     # preds = (preds - preds.min())/(preds.max()-preds.min())\n",
    "#     # print(test)\n",
    "#\n",
    "#\n",
    "# print(defect_preds)\n",
    "# print(np.mean(defect_preds))\n",
    "# true_def_pred = len(np.where(defect_preds > threshold)[0])\n",
    "# print(true_def_pred)\n",
    "#\n",
    "#\n",
    "# print(normal_preds)\n",
    "# print(np.mean(normal_preds))\n",
    "# true_nor_pred = len(np.where(normal_preds < threshold)[0])\n",
    "# print(true_nor_pred)\n",
    "#\n",
    "# total_acc = (true_def_pred + true_nor_pred) / (len_nor_data + len_def_data) * 100\n",
    "# print(\"total_accuracy: \", total_acc)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "51\n",
      "50\n",
      "[0.64748341 0.64582115 0.64495695 0.63706183 0.64389598 0.64372158\n",
      " 0.64029634 0.64423901 0.63870716 0.64056474 0.63585079 0.63704026\n",
      " 0.6411283  0.64476216 0.63866848 0.63880849 0.63715732 0.63976002\n",
      " 0.64145702 0.64556265 0.64365792 0.6378026  0.64362186 0.63863498\n",
      " 0.64006096 0.64166403 0.64609551 0.63979173 0.64554846 0.63616109\n",
      " 0.64451241 0.64017868 0.6469053  0.63884485 0.63761288 0.64289093\n",
      " 0.64132708 0.64631164 0.64095742 0.63797343 0.64088923 0.63647813\n",
      " 0.65117645 0.63967681 0.64133358 0.63783252 0.64289385 0.64198399\n",
      " 0.63981831 0.63435316]\n",
      "0.6412786686420441\n",
      "50\n",
      "[0.63939631 0.64166212 0.63711405 0.63674265 0.63656467 0.63669407\n",
      " 0.63940752 0.63682628 0.63999265 0.63952291 0.63734055 0.64020377\n",
      " 0.63860971 0.640733   0.64046967 0.63901877 0.63815737 0.63886672\n",
      " 0.6363433  0.63823372 0.63949871 0.63427675 0.63712913 0.64116955\n",
      " 0.63772613 0.63943613 0.63968712 0.6397562  0.63884401 0.64015615\n",
      " 0.63704967 0.64029896 0.63756126 0.63938802 0.63810068 0.63947988\n",
      " 0.63981718 0.6367234  0.63923091 0.63839173 0.63806874 0.63837016\n",
      " 0.64042914 0.64004749 0.6408875  0.63947093 0.63853627 0.63996935\n",
      " 0.63839281 0.63756096]\n",
      "0.6387470948696137\n",
      "0\n",
      "total_accuracy:  49.504950495049506\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "x3EaS4Uz6ZDV"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}